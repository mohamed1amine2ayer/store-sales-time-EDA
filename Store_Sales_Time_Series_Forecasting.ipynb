{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset Description**\n",
        "In this competition, you will predict sales for the thousands of product families sold at Favorita stores located in Ecuador. The training data includes dates, store and product information, whether that item was being promoted, as well as the sales numbers. Additional files include supplementary information that may be useful in building your models.\n",
        "\n",
        "**File Descriptions and Data Field Information**\n",
        "\n",
        "\n",
        "  ***train.csv***\n",
        "         \n",
        "          The training data, comprising time series of features store_nbr,family,and onpromotion as well as the target sales.\n",
        "          store_nbr identifies the store at which the products are sold\n",
        "          family identifies the type of product sold\n",
        "          sales gives the total sales for a product family at a particular store at a given date. Fractional values are possible since products can be sold in fractional units (1.5 kg of cheese, for instance, as opposed to 1 bag of chips).\n",
        "          sales gives the total sales for a product family at a particular store at a given date. Fractional values are possible since products can be sold in fractional units (1.5 kg of cheese, for instance, as opposed to 1 bag of chips).\n",
        "\n",
        "  ***test.csv***\n",
        "\n",
        "      The test data, having the same features as the training data. You will predict the target sales for the dates in this file.\n",
        "      The dates in the test data are for the 15 days after the last date in the training data\n",
        "\n",
        "  ***sample_submission.csv***\n",
        "\n",
        "      A sample submission file in the correct format\n",
        "  ***stores.csv***\n",
        "\n",
        "     Store metadata, including city, state, type, and cluster\n",
        "     cluster is a grouping of similar stores\n",
        "\n",
        "  ***oil.csv***\n",
        "\n",
        "     Daily oil price. Includes values during both the train and test data timeframes. (Ecuador is an oil-dependent country and it's economical health is highly vulnerable to shocks in oil prices\n",
        "\n",
        "     ***holidays_events.csv***\n",
        "      \n",
        "      Holidays and Events, with metadata\n",
        "      \n",
        "      NOTE: Pay special attention to the transferred column. A holiday that is transferred officially falls on that calendar day, but was moved to another date by the government. A transferred day is more like a normal day than a holiday. To find the day that it was actually celebrated, look for the corresponding row where type is Transfer. For example, the holiday Independencia de Guayaquil was transferred from 2012-10-09 to 2012-10-12, which means it was celebrated on 2012-10-12. Days that are type Bridge are extra days that are added to a holiday (e.g., to extend the break across a long weekend). These are frequently made up by the type Work Day which is a day not normally scheduled for work (e.g., Saturday) that is meant to payback the Bridge.\n",
        "       \n",
        "       Additional holidays are days added a regular calendar holiday, for example, as typically happens around Christmas (making Christmas Eve a holiday).\n",
        "\n",
        "  **Additional Notes**\n",
        "    \n",
        "     Wages in the public sector are paid every two weeks on the 15 th and on the last day of the month. Supermarket sales could be affected by this.\n",
        "\n",
        "     A magnitude 7.8 earthquake struck Ecuador on April 16, 2016. People rallied in relief efforts donating water and other first need products which greatly affected supermarket sales for several weeks after the earthquake\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mKjEThJwBhLD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgdYVsZkzREP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import pandas_profiling"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_Gfj_DvmBeYB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8AqyyKczLM6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install https://github.com/pandas-profiling/pandas-profiling/archive/master.zip"
      ],
      "metadata": {
        "id": "04vDabx53eDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas_profiling import ProfileReport\n"
      ],
      "metadata": {
        "id": "g05_ZCuh38QJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "oil=pd.read_csv('/content/oil.csv')\n",
        "sample_submission=pd.read_csv('/content/sample_submission.csv')\n",
        "stores=pd.read_csv('/content/stores.csv')\n",
        "test=pd.read_csv('/content/test.csv')\n",
        "train=pd.read_csv('/content/train.csv')\n",
        "transactions=pd.read_csv('/content/transactions.csv')\n",
        "holidays_events=pd.read_csv('/content/oil.csv')"
      ],
      "metadata": {
        "id": "2A6HIq2W4CVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "profile = ProfileReport(oil , html = {'style' : {'full_width':True}})\n",
        "profile.to_file(output_file=\"report.html\")\n",
        "profile.to_notebook_iframe()"
      ],
      "metadata": {
        "id": "Y1aUqq8t4DcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "profile = ProfileReport(train , html = {'style' : {'full_width':True}})\n",
        "profile.to_file(output_file=\"report.html\")\n",
        "profile.to_notebook_iframe()"
      ],
      "metadata": {
        "id": "iuLfXGtnKOFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E9dvk7zyw8x3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u1nAeoSow_pp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "profile = ProfileReport(stores , html = {'style' : {'full_width':True}})\n",
        "profile.to_file(output_file=\"report.html\")\n",
        "profile.to_notebook_iframe()"
      ],
      "metadata": {
        "id": "_ZZdHlavKfAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-PRKgUnhp3k0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SONkgm5-p5lX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "102HuI3dqAt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ApwnHLznBcMf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "M7YaKrw3BcYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os._exit(00)"
      ],
      "metadata": {
        "id": "mZ8JDjlN3kV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_submission.head(20)"
      ],
      "metadata": {
        "id": "Y_YwHRV2LR7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "profile = ProfileReport( transactions , html = {'style' : {'full_width':True}})\n",
        "profile.to_file(output_file=\"report.html\")\n",
        "profile.to_notebook_iframe()"
      ],
      "metadata": {
        "id": "3AGnR5xJMeHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#merge training  data with stores \n",
        "train_merged = pd.merge(train , stores , on='store_nbr')\n",
        "train_merged = train_merged.astype({'store_nbr' : 'str' ,'family' : 'str' ,'city' :'str' ,'state':'str' , 'type': 'str' , 'cluster' :'str'})\n",
        "\n",
        "# transactions data\n",
        "transactions_pivoted = pd.pivot_table(transactions , values ='transactions' , index='date' , columns = ['store_nbr']).reset_index().rename_axis(None , axis =1)\n",
        "transactions_pivoted = transactions_pivoted.rename(columns = {transactions_pivoted.columns[0] : 'date'})\n",
        "# test data \n",
        "test_dropped = test.drop(['onpromotion'] , axis = 1) \n",
        "test_dropped = test_dropped.sort_values(by = ['store_nbr' , 'family'])"
      ],
      "metadata": {
        "id": "NhM4WZQSxCkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_merged.head()"
      ],
      "metadata": {
        "id": "KtKlc5-r2AjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dropped.head()"
      ],
      "metadata": {
        "id": "5Gkbu3SV3Q-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transactions_pivoted.head()"
      ],
      "metadata": {
        "id": "tzl6nBSY3yt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install Darts"
      ],
      "metadata": {
        "id": "m_qwJMlZ8sFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Darts-specific TimeSeries Objects\n",
        "import darts\n",
        "import numpy as np\n",
        " # Transactions\n",
        "transactions_TS = darts.TimeSeries.from_dataframe(transactions_pivoted, \n",
        "                                            time_col = 'date',\n",
        "                                            fill_missing_dates=True, \n",
        "                                            freq='D',\n",
        "                                            fillna_value = 0)\n",
        "transactions_TS = transactions_TS.astype(np.float32)"
      ],
      "metadata": {
        "id": "Oj9OIN2E6BtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transactions_pivoted"
      ],
      "metadata": {
        "id": "jpey1JMqAxAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transactions_TS [1]"
      ],
      "metadata": {
        "id": "kM6EVwbVTLUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Data\n",
        "\n",
        "train_sequence = darts.TimeSeries.from_group_dataframe(\n",
        "    train_merged,\n",
        "    time_col=\"date\",\n",
        "    group_cols=[\"store_nbr\",\"family\"],  # individual time series are extracted by grouping `df` by `group_cols`\n",
        "    static_cols=[\"city\",\"state\",\"type\",\"cluster\"], # also extract these additional columns as static covariates\n",
        "    value_cols=\"sales\",\n",
        "    fill_missing_dates=True,\n",
        "    freq='D')\n",
        "\n",
        "for i in range(0,len(train_sequence)):\n",
        "    train_sequence[i] = train_sequence[i].astype(np.float32)\n",
        "    \n",
        "train_sequence = sorted(train_sequence, key=lambda ts: int(float(ts.static_covariates_values()[0,0])))\n",
        "\n"
      ],
      "metadata": {
        "id": "H1ZcKLW1Uzn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_merged[train_merged['cluster'].astype(np.float32) == 13]"
      ],
      "metadata": {
        "id": "Vnj05u7TVGU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_merged"
      ],
      "metadata": {
        "id": "9qoD76w0YU1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "! pip install matplotlib"
      ],
      "metadata": {
        "id": "nIzaftVeVMv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's print two of the 1782 TimeSeries\n",
        "\n",
        "\n",
        "\n",
        "plt.subplots(2, 2, figsize=(15, 6))\n",
        "plt.subplot(1, 2, 1) # row 1, col 2 index 1\n",
        "train_sequence[5].plot(label='Sales for {}'.format(train_sequence[5].static_covariates_values()[0,1], \n",
        "                                                train_sequence[5].static_covariates_values()[0,0],\n",
        "                                                train_sequence[5].static_covariates_values()[0,2]))\n",
        "\n",
        "train_sequence[600].plot(label='Sales for {}'.format(train_sequence[600].static_covariates_values()[0,1], \n",
        "                                                train_sequence[600].static_covariates_values()[0,0],\n",
        "                                                train_sequence[600].static_covariates_values()[0,2]))\n",
        "\n",
        "plt.title(\"Two Out Of 1782 TimeSeries\")\n",
        "           \n",
        "plt.subplot(1, 2, 2) # index 2\n",
        "train_sequence[5][-365:].plot(label='Sales for {}'.format(train_sequence[5].static_covariates_values()[0,1], \n",
        "                                                train_sequence[5].static_covariates_values()[0,0],\n",
        "                                                train_sequence[5].static_covariates_values()[0,2]))\n",
        "\n",
        "train_sequence[600][-365:].plot(label='Sales for {}'.format(train_sequence[600].static_covariates_values()[0,1], \n",
        "                                                train_sequence[600].static_covariates_values()[0,0],\n",
        "                                                train_sequence[600].static_covariates_values()[0,2]))\n",
        "\n",
        "plt.title(\"Only The Last 365 Days\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "10nWi9E2V-26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RiK5xb2-Xox8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pu0qYTBQfvqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Nhx0wtM5TfLs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_sequence[50].static_covariates_values()"
      ],
      "metadata": {
        "id": "cGUatZPGf6Cn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from darts.utils.statistics import plot_acf, check_seasonality\n",
        "from darts.utils.missing_values import fill_missing_values\n",
        "\n",
        "plot_acf(fill_missing_values(train_sequence[5]), m=7, alpha=0.05)\n",
        "plt.title(\"{}, store {} in {}\".format(train_sequence[5].static_covariates_values()[0,1], \n",
        "                                                train_sequence[5].static_covariates_values()[0,0],\n",
        "                                                train_sequence[5].static_covariates_values()[0,2]))\n",
        "\n",
        "plot_acf(fill_missing_values(train_sequence[600]), alpha=0.05)\n",
        "plt.title(\"{}, store {} in {}\".format(train_sequence[600].static_covariates_values()[0,1], \n",
        "                                                train_sequence[600].static_covariates_values()[0,0],\n",
        "                                                train_sequence[600].static_covariates_values()[0,2]))"
      ],
      "metadata": {
        "id": "dy421BfGXqZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, the BREAD/BAKERY series displays strong weekly seasonality, as we would expect. The CELEBRATION series however has a much less clear seasonal pattern.\n",
        "\n",
        "In the next step, we encode the static covariates and apply 0-1 Scaling + Log-Transformation to all series:"
      ],
      "metadata": {
        "id": "6bXRGCzhlSZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from darts import TimeSeries\n",
        "from darts.models import NaiveSeasonal, ExponentialSmoothing, Prophet\n",
        "from darts.metrics import rmsle\n",
        "from darts.utils.timeseries_generation import datetime_attribute_timeseries\n",
        "from darts.dataprocessing.transformers import Scaler,MissingValuesFiller,StaticCovariatesTransformer,InvertibleMapper\n",
        "from darts.dataprocessing import Pipeline\n",
        "from tqdm import tqdm\n",
        "\n",
        "import sklearn\n",
        "from sklearn import preprocessing\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import gc\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "gS3oeirIqjP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Pre-Processing\n",
        "\n",
        "# Encode Static Covariates\n",
        "\n",
        "static_cov_transformer = StaticCovariatesTransformer(transformer_cat = sklearn.preprocessing.OrdinalEncoder()) #OneHot would be better, but takes much longer\n",
        "static_cov_transformed = static_cov_transformer.fit_transform(train_sequence)\n",
        "\n",
        "for i, (ts, ts_scaled) in enumerate(zip(train_sequence[32:33], static_cov_transformed[32:33])):\n",
        "    print(f\"Original series {i}\")\n",
        "    print(ts.static_covariates)\n",
        "    print(f\"Transformed series {i}\")\n",
        "    print(ts_scaled.static_covariates)\n",
        "    print(\"\")\n",
        "    \n",
        "\n",
        "\n",
        "train_filler =MissingValuesFiller(verbose=False, n_jobs=-1, name=\"Faster Filler\")\n",
        "\n",
        "log_transformer = InvertibleMapper(np.log1p, np.expm1, verbose=False, n_jobs=-1, name=\"Faster Log\")   \n",
        "\n",
        "train_scaler = Scaler(verbose=False, n_jobs=-1, name=\"Faster Scaler\")\n",
        "\n",
        "train_pipeline = Pipeline([train_filler, \n",
        "                           log_transformer, \n",
        "                           train_scaler])\n",
        "training_transformed = train_pipeline.fit_transform(static_cov_transformed)\n",
        "\n",
        "# Differencing the Series\n",
        "\n",
        "plt.subplots(2, 2, figsize=(15, 6))\n",
        "plt.subplot(1, 2, 1) # row 1, col 2 index 1\n",
        "training_transformed[4].plot(label='Sales for {}'.format(train_sequence[5].static_covariates_values()[0,1], \n",
        "                                                train_sequence[5].static_covariates_values()[0,0],\n",
        "                                                train_sequence[5].static_covariates_values()[0,2]))\n",
        "\n",
        "plt.title(\"TimeSeries After Scaling and Log-Transform\")\n",
        "           \n",
        "plt.subplot(1, 2, 2) # index 2\n",
        "training_transformed[5][-365:].plot(label='Sales for {}'.format(train_sequence[5].static_covariates_values()[0,1], \n",
        "                                                train_sequence[5].static_covariates_values()[0,0],\n",
        "                                                train_sequence[5].static_covariates_values()[0,2]))\n",
        "\n",
        "plt.title(\"Only The Last 365 Days\")\n",
        "plt.show()\n",
        "\n",
        "plt.subplots(2, 2, figsize=(15, 6))\n",
        "plt.subplot(1, 2, 1) # row 1, col 2 index 1\n",
        "training_transformed[600].plot(label='Sales for {}'.format(train_sequence[600].static_covariates_values()[0,1], \n",
        "                                                train_sequence[600].static_covariates_values()[0,0],\n",
        "                                                train_sequence[600].static_covariates_values()[0,2]))\n",
        "\n",
        "plt.title(\"TimeSeries After Scaling and Log-Transform\")\n",
        "           \n",
        "plt.subplot(1, 2, 2) # index 2\n",
        "training_transformed[600][-365:].plot(label='Sales for {}'.format(train_sequence[600].static_covariates_values()[0,1], \n",
        "                                                train_sequence[600].static_covariates_values()[0,0],\n",
        "                                                train_sequence[600].static_covariates_values()[0,2]))\n",
        "\n",
        "plt.title(\"Only The Last 365 Days\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "M7QsVUuJqGpV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}